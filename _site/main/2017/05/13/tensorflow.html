<!DOCTYPE html>
<html lang="en-us">
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <head>
  <meta charset="UTF-8">
  <title>Standing on Giant's Shoulder</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <link rel="stylesheet" href="/css/normalize.css">
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="/css/cayman.css">
</head>

  <body>
    <section class="page-header">
  <h1 class="project-name">Standing on Giant's Shoulder</h1>
  <h2 class="project-tagline"></h2>

  <a href="https://github.com/keeperovswords/Tensorflow_CIFAR" class="btn">View on GitHub</a>
  <a href="https://github.com/keeperovswords/Tensorflow_CIFAR/archive/gh-pages.zip" class="btn">Download .zip</a>
  <!--
  <a href="#" class="btn">Download .tar.gz</a>
  -->
</section>


    <section class="main-content">
      
      <h2>Tensorflow</h2>
<p class="meta">13 May 2017</p>

<p>Tensorflow (TF) is open-source library for numerical computation for machine intelligence. It’s developed by Google and used for working on Google Brain project. TF has also some speciality. It uses the computational graph to build scalable models to one or more CPUs and GPUs. All the operations are represented as nodes in computational graph and the data containers such as tensor, matrix, vector are assigned to the edge of this graph. Formally to say the graph embodies the necessary parts for computations.</p>

<h1>Syntax</h1>
<p>All the nodes and edges only has computational mean when it run in a session that encapsulates the running time of a computational model. It has different variable types. The basic type i.e. constant that holds a constant in nodes. For instance we initialized a constant variable <script type="math/tex">c</script> in TF is defined as follows.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">...</span>
<span class="c"># Import Tensorflow</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
	
<span class="c"># Constant node in graph</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c"># Variable will get values later</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="c"># Operator in graph</span>
<span class="n">add_node</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="c"># Trainable variable with initial value and data type</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">([</span><span class="o">.</span><span class="mi">3</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c"># Runtime encapsulator for TF</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>

<span class="c"># Initial all variables </span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
<span class="o">...</span></code></pre></figure>

<p>Another variable is <script type="math/tex">placeholder</script> that means the value of this variable will be assigned later liked we defined <script type="math/tex">a, b</script>.
The addition operator defined just like in normal computer language. The <script type="math/tex">Variable</script> makes this variable learnable. As a session initialized, then we can invoke its run function to launch our computation.</p>

<h1>Working on TF</h1>
<p>After a brief syntax introduction in TF we’d like use it to experience how it work in image classification on CIFAR-10 data. The process is quite easy, for simplicity you can use <script type="math/tex">tf.layer</script> to build your computational graph, but I’d like to build it more natively. The follows structure will be used in our model:</p>
<ul>
<li>7x7 convolutional layer with 32 filters and stride of 1</li>
<li>ReLU activation layer</li>
<li>spatial batch normalization Layer (trainable parameters, with scale and centering)</li>
<li>2x2 max pooling layer with a stride of 2</li>
<li>affine layer with 1024 output units</li>
<li>ReLU activation layer</li>
<li>affine layer from 1024 input units to 10 outputs. </li>
</ul>

<p>The inputs will be convoluted with convolution layer, so we need both weight and bias for this layer. As the input images have three channels (<script type="math/tex">R,G,B</script>) , so the weight tensor in TF is organized as [7, 7, 3, 32], each filter has a bias parameter. The output of convolution layer can be either in batch normalization or ReLU layer fed.  After it the output in ReLU will be max sub-pooled in size 2x2 with a stride of 2. The output of sub-pooling layer will be fully connected with a feed forward neuron layer. With ReLU it will activated and  be connected in the output layer with 10 neuron. This computation graph in the following figure. It explains clearly how the graph connected and how the data flows.</p>
<div class="fig figcenter fighighlight">
  <img src="http://github.com/pages/keeperovswords/Tensorflow_CIFAR/assets/graph_run.png" width="75%" />
</div>
<p>This graph can be built in the following code piece:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">...</span>
<span class="c">#7x7 Convolutional Layer with 32 filters and stride of 1</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'conv1'</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'weight'</span><span class="p">):</span>
    <span class="n">WC1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">"weight_conv"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>
    <span class="n">variable_summaries</span><span class="p">(</span><span class="n">WC1</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'bias'</span><span class="p">):</span>
  <span class="n">bc1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">"bias_conv"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">])</span>
  <span class="n">variable_summaries</span><span class="p">(</span><span class="n">bc1</span><span class="p">)</span>

<span class="c"># Convoluton layer</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'conv2d'</span><span class="p">):</span>
  <span class="n">h_conv1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">WC1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s">'VALID'</span><span class="p">)</span> <span class="o">+</span> <span class="n">bc1</span>
  <span class="k">print</span> <span class="p">(</span><span class="s">"h_conv1.shape"</span><span class="p">,</span><span class="n">h_conv1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c"># Batch normalization layer</span>
<span class="c">#with tf.name_scope('batch_norm'):</span>
<span class="c">#    batched = batch_normalization(h_conv1, 32, is_training, 'batch_norm')</span>

<span class="c"># ReLU activation nayer</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'ReLU'</span><span class="p">):</span>
  <span class="c">#h1_relu = tf.nn.relu(h_conv1)</span>
  <span class="n">h1_relu</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">h_conv1</span><span class="p">)</span>
  <span class="k">print</span> <span class="p">(</span><span class="s">"h1_relu.shape"</span><span class="p">,</span><span class="n">h1_relu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c"># 2x2 Max Pooling layer with a stride of 2</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'pooling'</span><span class="p">):</span>
  <span class="n">h_pool1</span> <span class="o">=</span> <span class="n">max_pool_2x2</span><span class="p">(</span><span class="n">h1_relu</span><span class="p">)</span>
  <span class="k">print</span> <span class="p">(</span><span class="s">"h_pool1.shape"</span><span class="p">,</span><span class="n">h_pool1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'flatten'</span><span class="p">):</span>
  <span class="n">h_pool1_flat</span><span class="p">,</span> <span class="n">channels</span> <span class="o">=</span> <span class="n">conv2d_flatten</span><span class="p">(</span><span class="n">h_pool1</span><span class="p">)</span>
  <span class="k">print</span> <span class="p">(</span><span class="s">"h_pool1_flat.shape"</span><span class="p">,</span><span class="n">h_pool1_flat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'affine1'</span><span class="p">):</span>

  <span class="c">#with tf.name_scope('batch_norm'):</span>
  <span class="c">#    batched = tf.nn.batch_normalization(h_pool1, 0, 1, )</span>

  <span class="c"># Affine layer with 1024 output units</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'weight'</span><span class="p">):</span>
    <span class="n">WFC1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">"dense_wight1"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">channels</span><span class="p">,</span> <span class="mi">1024</span><span class="p">])</span>
    <span class="n">variable_summaries</span><span class="p">(</span><span class="n">WFC1</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'bias'</span><span class="p">):</span>
    <span class="n">bfc1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">"dense_bias1"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1024</span><span class="p">])</span>
    <span class="n">variable_summaries</span><span class="p">(</span><span class="n">bfc1</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'output'</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_pool1_flat</span><span class="p">,</span> <span class="n">WFC1</span><span class="p">)</span> <span class="o">+</span> <span class="n">bfc1</span>

  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'batch_norm'</span><span class="p">):</span>
    <span class="n">batched</span> <span class="o">=</span> <span class="n">batch_normalization</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="s">'batch_norm'</span><span class="p">)</span>

  <span class="c"># ReLU Activation Layer</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'ReLU'</span><span class="p">):</span>
    <span class="n">h_fc1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">batched</span><span class="p">)</span>
    <span class="n">variable_summaries</span><span class="p">(</span><span class="n">h_fc1</span><span class="p">)</span>
    <span class="k">print</span> <span class="p">(</span><span class="s">"h_fc1.shape"</span><span class="p">,</span><span class="n">h_fc1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'affine2'</span><span class="p">):</span>
  <span class="c"># Affine layer from 1024 input units to 10 outputs</span>
  <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'weight'</span><span class="p">):</span>
  <span class="n">WFC2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">"dense_wight2"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
  <span class="n">variable_summaries</span><span class="p">(</span><span class="n">WFC2</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'bias'</span><span class="p">):</span>
  <span class="n">bfc2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">"dense_bias2"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">])</span>
  <span class="n">variable_summaries</span><span class="p">(</span><span class="n">bfc2</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'output'</span><span class="p">):</span>
  <span class="n">y_out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">h_fc1</span><span class="p">,</span><span class="n">WFC2</span><span class="p">)</span> <span class="o">+</span> <span class="n">bfc2</span>
  <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="s">'activations'</span><span class="p">,</span> <span class="n">y_out</span><span class="p">)</span>
<span class="o">...</span></code></pre></figure>

<p>After convolution the activated featuers are as inputs in dense neural layer applied. The output of this dense layer is connected to the output layer. The whole graph will be launched with calling <script type="math/tex">session.run()</script>.</p>

<h2>Softmax</h2>
<p>The softmax in output layer is calculated as follows:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c"># define our loss</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">):</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">softmax_cross_entropy</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span><span class="n">logits</span><span class="o">=</span><span class="n">y_out</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'cross_entropy'</span><span class="p">,</span> <span class="n">total_loss</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="s">'loss'</span><span class="p">):</span>
    <span class="n">mean_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'mean_loss'</span><span class="p">,</span> <span class="n">mean_loss</span><span class="p">)</span></code></pre></figure>

<p><script type="math/tex">logits</script> needs the output in our graph, and <script type="math/tex">y</script> is the ground truth.</p>

<h2>Optimizer</h2>
<p>Compared with optimizer we used before, in TF it involves just a API call. At first get the update collections, then you assign the optimizer you use, the learning rate for optimizer and what you want to minimize, here we intend to minimize the mean_loss we calculated before.</p>

<figure class="highlight"><pre><code class="language-pythin" data-lang="pythin">with tf.name_scope('Opts'):
    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    
    with tf.control_dependencies(extra_update_ops):
        train_step = tf.train.AdamOptimizer(1e-4).minimize(mean_loss)</code></pre></figure>

<h2></h2>

<h1>Visualize Graph</h1>
<p>With <a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">TensorBoard</a> you can check your computaional graph that we built before. To achieve this you should do a little more works as usual, but it’s also quite simple.
Before you seesion is launched, you should assign the log path to save your computational logs.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">...</span>
<span class="n">log_path_train</span> <span class="o">=</span> <span class="s">'VisualBoard/train'</span>
<span class="n">log_path_test</span> <span class="o">=</span> <span class="s">'VisualBoard/test'</span>
<span class="o">...</span></code></pre></figure>

<p>Those two lines create two folders named train and test separately in directory VisualBoard. TensorBoard will parse the log your graph generated and summaries it to build the graph you want to check.
After running the code to build graph, it still does n’t exist untill you call the <script type="math/tex">session.run()</script> API.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="c"># Tensorflow will gather all the summary information for visualization later </span>
    <span class="n">merged</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">merge_all</span><span class="p">()</span>

    <span class="c"># Where the train/test log goes to</span>
    <span class="n">train_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">FileWriter</span><span class="p">(</span><span class="n">log_path_train</span><span class="p">,</span> <span class="n">sess</span><span class="o">.</span><span class="n">graph</span><span class="p">)</span>
    <span class="n">test_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">FileWriter</span><span class="p">(</span><span class="n">log_path_test</span><span class="p">)</span>

    <span class="c"># Using GPU or CPU to run session "/cpu:0" or "/gpu:0" </span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"/gpu:0"</span><span class="p">):</span>
        <span class="c"># Initialize all the global variables firstly</span>
        <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
        
        <span class="c"># run you graph in minibatch model </span>
        <span class="n">run_model</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span><span class="n">y_out</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="n">train_step</span><span class="p">,</span><span class="bp">True</span><span class="p">)</span></code></pre></figure>

<p>Just one last step to visualize the graph. After running you model out, you should invoke the visualization command to build the image we saw before.
Using the following command in terminal window</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">tensorboard</span> <span class="o">--</span><span class="n">logdir</span><span class="o">=</span><span class="n">VisualBoard</span><span class="o">/</span><span class="n">train</span></code></pre></figure>

<p>Here we go, you will get a windows like this:</p>
<div class="fig figcenter fighighlight">
  <img src="http://github.com/pages/keeperovswords/Tensorflow_CIFAR/assets/tensorboard.png" width="75%" />
</div>
<p>Just click the http addres you will see the graph we saw before.</p>

<h1>Conclusion</h1>
<p>As you can see,  building a deep learning model in Tensorflow is quite consice. What you need is define the forward propagation. After it, you can kick-off the run time of compuation session to let you compuataional graph comes alive. You can pull the whole project and have a try with TF! Have fun with that!</p>



      <footer class="site-footer">
  <span class="site-footer-owner"><a href="http://localhost:4000">Standing on Giant's Shoulder</a> is maintained by <a href="https://github.com/keeperovswords">Jian Xi</a>.</span>
  <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
</footer>


    </section>

  </body>
</html>
